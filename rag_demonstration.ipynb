{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete RAG Demonstration: From Naive to Advanced\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** is a technique that enhances LLM responses by retrieving relevant documents from a knowledge base before generating an answer. Instead of relying solely on the LLM's training data, RAG allows the model to access current, domain-specific information.\n",
    "\n",
    "**Why RAG?**\n",
    "- LLMs have knowledge cutoff dates and don't know about recent information\n",
    "- LLMs don't know about your private/company data\n",
    "- RAG grounds responses in actual documents, reducing hallucinations\n",
    "\n",
    "This notebook demonstrates RAG using **LangChain v1+ with LCEL** (LangChain Expression Language).\n",
    "\n",
    "**Key Technologies:**\n",
    "- **LangChain**: Framework for building LLM applications\n",
    "- **LCEL**: Modern way to compose LangChain components as chains\n",
    "- **sentence-transformers**: Converts text to vector embeddings\n",
    "- **Qdrant**: Vector database for similarity search\n",
    "- **LangGraph**: Orchestrates complex multi-step workflows\n",
    "- **Ollama**: Runs LLMs locally (we'll use Gemma2 2B)\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup\n",
    "2. LLM Without RAG\n",
    "3. Naive RAG\n",
    "4. Where Naive RAG Fails\n",
    "5. Advanced RAG (LangGraph)\n",
    "6. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### Prerequisites\n",
    "```bash\n",
    "# Install UV\n",
    "brew install uv\n",
    "\n",
    "# Install Ollama\n",
    "brew install ollama\n",
    "ollama serve\n",
    "ollama pull gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!uv pip install langchain-core langchain-community langgraph langchain-ollama\n",
    "!uv pip install qdrant-client langchain-qdrant sentence-transformers langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Qdrant Vector Database\n",
    "\n",
    "**Qdrant** is a vector database optimized for similarity search. Unlike traditional databases that store rows of data, vector databases store embeddings (numerical representations of text) and can quickly find similar items.\n",
    "\n",
    "We'll run Qdrant in a Docker container for easy setup and teardown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3f08c8da6eee5d271c3e05aaa5245207935fee31223aadc9d1e4abb05ecf816\n",
      "[OK] Qdrant assumed running at http://localhost:6333/dashboard\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start Qdrant container\n",
    "!docker run -d -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    --name qdrant_rag qdrant/qdrant:latest\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"[OK] Qdrant assumed running at http://localhost:6333/dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM, Embeddings, and Database Client\n",
    "\n",
    "We need three key components:\n",
    "\n",
    "1. **LLM (Large Language Model)**: Gemma2 2B via Ollama - generates human-readable answers\n",
    "2. **Embedding Model**: sentence-transformers - converts text to vectors (384-dimensional) for similarity search\n",
    "3. **Qdrant Client**: Connects to our vector database\n",
    "\n",
    "**Important distinction**: The embedding model (22M parameters) only converts text to vectors, while the LLM (2B parameters) generates text. They serve different purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] LLM: Gemma2 2B (Ollama)\n",
      "[OK] Embeddings: all-MiniLM-L6-v2 (384d)\n",
      "[OK] Qdrant: collections=[]\n"
     ]
    }
   ],
   "source": [
    "# Import models and clients\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize LLM (for generating text)\n",
    "llm = OllamaLLM(model=\"gemma2:2b\", temperature=0.1)\n",
    "print(\"[OK] LLM: Gemma2 2B (Ollama)\")\n",
    "\n",
    "# Initialize embedding model (for converting text to vectors)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"[OK] Embeddings: all-MiniLM-L6-v2 (384d)\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "print(f\"[OK] Qdrant: {qdrant_client.get_collections()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "We'll create a small knowledge base about a fictional restaurant (The Bangalore Bistro). This simulates having internal restaurant documents (menu, policy, hours) that aren't in the LLM's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 10 detailed documents, 2 test questions\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"\"\"The Whitefield Vision (Doc 1): \n",
    "    We are proud to announce the grand opening of 'Bangalore Bistro: Whitefield' on February 15, 2026. \n",
    "    This branch marks the beginning of our 'Robo-Server' era, featuring sleek metallic decor and a \n",
    "    fully automated dining experience designed specifically for the tech-forward community of Whitefield.\"\"\",\n",
    "    \n",
    "    \"\"\"The Signature Avocado Toast (Doc 2): \n",
    "    Our 'Green Gold' Avocado Toast (‚Çπ450) is a fan favorite. It features slow-fermented sourdough \n",
    "    bread from our Indiranagar bakery, topped with hand-picked Hass avocados, pomegranate seeds, \n",
    "    and a drizzle of our signature spicy local chili oil.\"\"\",\n",
    "    \n",
    "    \"\"\"The Heritage Filter Coffee (Doc 3): \n",
    "    Experience our Filter Coffee (‚Çπ60), crafted from a 50-year-old family recipe using Peaberry \n",
    "    beans from Chikmagalur. Brewed in traditional brass filters, it delivers the perfect, frothy \n",
    "    decoction that Bangaloreans have loved for decades.\"\"\",\n",
    "    \n",
    "    \"\"\"Senior Citizen Inclusivity (Doc 4): \n",
    "    We honor our elders with a flat 20% discount on all food items for guests aged 65 and above. \n",
    "    Simply present a valid government ID to our staff (or scan it at the Robo-Kiosk) to avail of \n",
    "    this benefit. Respecting our roots is a core value at the Bistro.\"\"\",\n",
    "    \n",
    "    \"\"\"The Early Bird Rewards (Doc 5): \n",
    "    Start your productivity early at any of our branches. All orders placed before 9:00 AM are \n",
    "    eligible for a 15% 'Early Bird' discount. It's our way of rewarding the early risers and \n",
    "    ensuring a peaceful, budget-friendly start to your day.\"\"\",\n",
    "    \n",
    "    \"\"\"The Robo-Tech Surcharge (Doc 6): \n",
    "    To maintain our innovative fleet of automated servers at the Whitefield branch, a 10% \n",
    "    'Robo-Tech service fee' is added to every bill. This fee ensures peak performance, \n",
    "    regular maintenance, and a seamless futuristic dining experience for all guests.\"\"\",\n",
    "    \n",
    "    \"\"\"Membership Evolution (Doc 7): \n",
    "    With the launch of our app, the traditional 'Gold Leaf Membership Card' is now retired. \n",
    "    Previous cardholders can verify their details on the new 'Bistro App' to receive an \n",
    "    immediate ‚Çπ200 joining credit, which can be applied toward any future bill.\"\"\",\n",
    "    \n",
    "    \"\"\"Whitefield Safety Protocols (Doc 8): \n",
    "    Due to the high-voltage automated equipment and moving parts of our robotic servers, the \n",
    "    Whitefield branch is strictly NOT pet-friendly. We prioritize the safety of both our \n",
    "    robotic staff and your beloved furry companions.\"\"\",\n",
    "    \n",
    "    \"\"\"The Indiranagar Heritage (Doc 9): \n",
    "    Our Indiranagar branch continues to be a pet-friendly oasis, offering lush garden seating \n",
    "    and special 'Puppy-Patties' for our four-legged guests. It remains the perfect weekend \n",
    "    retreat for families and their pets in the heart of the city.\"\"\",\n",
    "    \n",
    "    \"\"\"Senior Launch Special (Doc 10): \n",
    "    For the grand opening of the Whitefield branch on Feb 15, 2026, all eligible senior \n",
    "    citizens (65+) will receive a complimentary Filter Coffee. This gesture symbolizes our \n",
    "    dedication to bridging traditional hospitality with the future of dining.\"\"\"\n",
    "]\n",
    "\n",
    "test_questions = [\n",
    "    \"When does the Bangalore Bistro's Whitefield branch open?\",\n",
    "    \"Is the Indiranagar branch pet-friendly?\"\n",
    "]\n",
    "\n",
    "print(f\"[OK] {len(documents)} detailed documents, {len(test_questions)} test questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Without RAG\n",
    "\n",
    "First, let's see what happens when we ask the LLM directly without providing any context. The LLM will only use its training data, which doesn't include our The Bangalore Bistro restaurant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLM WITHOUT RAG\n",
      "================================================================================\n",
      "\n",
      "Q: When does the Bangalore Bistro's Whitefield branch open?\n",
      "A: I do not have access to real-time information, including business hours. \n",
      "\n",
      "To find out when the Bangalore Bistro's Whitefield branch opens, I recommend:\n",
      "\n",
      "* **Checking their website:** Most businesses list their operating hours on their official website.\n",
      "* **Calling them directly:** You can call the restaurant and ask about their opening times.\n",
      "* **Using a food delivery app:** Apps like Zomato or Swiggy often display the business hours of restaurants in their listings. \n",
      "\n",
      "\n",
      "I hope this helps! \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: Is the Indiranagar branch pet-friendly?\n",
      "A: I do not have access to real-time information, including details about specific businesses like whether they are pet-friendly. \n",
      "\n",
      "**To find out if the Indiranagar branch of a particular business is pet-friendly, I recommend:**\n",
      "\n",
      "* **Checking their website:** Many businesses list their policies on their websites.\n",
      "* **Calling the branch directly:** This is the most reliable way to get an accurate answer.\n",
      "* **Looking for reviews online:** Websites like Yelp or Google often have customer reviews that mention pet policy. \n",
      "\n",
      "\n",
      "Please provide me with the name of the business you are interested in, and I can help you find their contact information or website! \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Analysis] LLM cannot provide accurate TechCorp-specific information\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LLM WITHOUT RAG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {llm.invoke(q)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n[Analysis] LLM cannot provide accurate TechCorp-specific information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive RAG\n",
    "\n",
    "Now let's implement a basic RAG system:\n",
    "1. **Split** documents into chunks\n",
    "2. **Embed** chunks into vectors and **store** in Qdrant\n",
    "3. **Retrieve** relevant chunks based on question similarity\n",
    "4. **Generate** answer using LLM with retrieved context\n",
    "\n",
    "### Step 1: Prepare and Store Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 10 chunks\n",
      "[OK] Qdrant collection: bangalore_bistro_2026\n",
      "[OK] Retriever created (k=2)\n"
     ]
    }
   ],
   "source": [
    "# Import document processing tools\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Split documents into chunks (smaller pieces for better retrieval)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# 1. Fixed chunk length\n",
    "# 2. split based on para\n",
    "# 3. intelligent split\n",
    "doc_objects = [Document(page_content=doc) for doc in documents]\n",
    "splits = text_splitter.split_documents(doc_objects)\n",
    "print(f\"[OK] {len(splits)} chunks\")\n",
    "\n",
    "# Create vector store in Qdrant\n",
    "collection_name = \"bangalore_bistro_2026\"\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)  # Clean up previous runs\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Store document embeddings in Qdrant\n",
    "vectorstore = QdrantVectorStore.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    url=\"http://localhost:6333\",\n",
    "    prefer_grpc=False,\n",
    "    collection_name=collection_name,\n",
    ")\n",
    "print(f\"[OK] Qdrant collection: {collection_name}\")\n",
    "\n",
    "# Create retriever (will fetch top k=2 most similar chunks)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "print(\"[OK] Retriever created (k=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build RAG Chain with LCEL\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is the modern v1+ way to compose chains. Think of it as building a pipeline where:\n",
    "- The `|` operator chains components together\n",
    "- Data flows from left to right\n",
    "- Each component transforms the data\n",
    "\n",
    "Our chain: `question ‚Üí retrieve docs ‚Üí format ‚Üí prompt ‚Üí LLM ‚Üí parse output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Naive RAG chain created\n"
     ]
    }
   ],
   "source": [
    "# Import LCEL components\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"Answer based on context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build chain using LCEL syntax\n",
    "naive_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,      # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass question through\n",
    "    }\n",
    "    | prompt                                   # Fill prompt template\n",
    "    | llm                                      # Generate answer\n",
    "    | StrOutputParser()                       # Parse to string\n",
    ")\n",
    "\n",
    "print(\"[OK] Naive RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NAIVE RAG (k=2)\n",
      "================================================================================\n",
      "\n",
      "Q: When does the Bangalore Bistro's Whitefield branch open?\n",
      "A: The Bangalore Bistro's Whitefield branch opens on **February 15, 2026**. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: Is the Indiranagar branch pet-friendly?\n",
      "A: Yes, the Indiranagar branch is pet-friendly.  The text states it offers \"lush garden seating\" and \"Puppy-Patties\" for pets. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Success] Naive RAG provides accurate answers!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NAIVE RAG (k=2)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = naive_rag_chain.invoke(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n[Success] Naive RAG provides accurate answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Where Naive RAG Fails\n",
    "\n",
    "Naive RAG struggles with:\n",
    "- **Multi-hop questions**: Requiring information from multiple documents (e.g., combining menu + offers)\n",
    "- **Vague questions**: Questions that need context from different sections\n",
    "- **Limited retrieval**: Only fetching k=2 documents might miss relevant information\n",
    "\n",
    "Let's test with slightly more complex questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTREME RAG DIFFERENTIATION\n",
      "================================================================================\n",
      "\n",
      "Q: I‚Äôm 70 years old, have a Gold Leaf Card and a pet dog. I want to visit the new branch on its opening day at 8:30 AM for Avocado Toast and Filter Coffee. Can I bring my dog, and what is my final cost using my credit?\n",
      "A: Based on the provided context, here's how we can answer your question:\n",
      "\n",
      "* **Bringing Your Dog:**  The text states that you can \"start your productivity early at any of our branches.\" This suggests that pets are generally allowed. However, it's always best to call the branch directly to confirm their pet policy. \n",
      "* **Final Cost:** You will receive a ‚Çπ200 joining credit when you verify your Gold Leaf Membership Card on the Bistro App.  You can use this credit towards your Avocado Toast and Filter Coffee order.\n",
      "\n",
      "**To get the exact final cost, follow these steps:**\n",
      "\n",
      "1. **Call the branch:** Confirm if they have any restrictions on bringing dogs to the opening day.\n",
      "2. **Check menu prices:** Find out the price of the Avocado Toast and Filter Coffee at the new branch. \n",
      "3. **Apply credit:** Use your ‚Çπ200 joining credit towards your order, leaving you with a final cost that is less than the total bill amount.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions! \n",
      "\n",
      "\n",
      "Retrieved Chunks (k=2):\n",
      "  1. Membership Evolution (Doc 7): \n",
      "    With the launch of our app, the traditional 'Gold Leaf Membership Card' is now retired. \n",
      "    Previous cardholders c...\n",
      "  2. The Early Bird Rewards (Doc 5): \n",
      "    Start your productivity early at any of our branches. All orders placed before 9:00 AM are \n",
      "    eligible for a 15...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: Which branch should I go to if I want to bring my pet for breakfast?\n",
      "A: You should go to the **Indiranagar** branch if you want to bring your pet for breakfast.  The Indiranagar branch is explicitly stated as being pet-friendly. \n",
      "\n",
      "\n",
      "Retrieved Chunks (k=2):\n",
      "  1. The Indiranagar Heritage (Doc 9): \n",
      "    Our Indiranagar branch continues to be a pet-friendly oasis, offering lush garden seating \n",
      "    and special 'Pup...\n",
      "  2. Whitefield Safety Protocols (Doc 8): \n",
      "    Due to the high-voltage automated equipment and moving parts of our robotic servers, the \n",
      "    Whitefield bra...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: How much out-of-pocket will I pay for an Avocado Toast at Whitefield if I use my retired membership credit?\n",
      "A: The provided context states the Signature Avocado Toast costs ‚Çπ450.  Since you have a retired 'Bistro App' membership card, and it comes with a ‚Çπ200 joining credit, you will only need to pay **‚Çπ250** for the Avocado Toast. \n",
      "\n",
      "\n",
      "Retrieved Chunks (k=2):\n",
      "  1. The Signature Avocado Toast (Doc 2): \n",
      "    Our 'Green Gold' Avocado Toast (‚Çπ450) is a fan favorite. It features slow-fermented sourdough \n",
      "    bread fro...\n",
      "  2. Membership Evolution (Doc 7): \n",
      "    With the launch of our app, the traditional 'Gold Leaf Membership Card' is now retired. \n",
      "    Previous cardholders c...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "challenging_questions = [\n",
    "    # The 9-Hop Extreme Mega-Query\n",
    "    # Needs Docs 1, 2, 3, 4, 5, 6, 7, 8, 10 for a perfect answer.\n",
    "    \"I‚Äôm 70 years old, have a Gold Leaf Card and a pet dog. I want to visit the new branch on its opening day at 8:30 AM for Avocado Toast and Filter Coffee. Can I bring my dog, and what is my final cost using my credit?\",\n",
    "    \n",
    "    # Ambiguity check: Needs Doc 8 vs Doc 9\n",
    "    \"Which branch should I go to if I want to bring my pet for breakfast?\",\n",
    "    \n",
    "    # Implicit link: Needs Doc 7 and Doc 6/2 for logic\n",
    "    \"How much out-of-pocket will I pay for an Avocado Toast at Whitefield if I use my retired membership credit?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTREME RAG DIFFERENTIATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for q in challenging_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = naive_rag_chain.invoke(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    \n",
    "    docs = vectorstore.similarity_search(q, k=2)\n",
    "    print(f\"\\nRetrieved Chunks (k=2):\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"  {i}. {doc.page_content[:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG with LangGraph\n",
    "\n",
    "**LangGraph** allows us to build stateful, multi-step workflows. Our advanced RAG adds:\n",
    "\n",
    "1. **Query Enhancement**: Rephrases questions with synonyms to improve retrieval\n",
    "2. **More Retrieval**: Fetches k=5 documents instead of k=2\n",
    "3. **Self-Correction**: Detects uncertain answers\n",
    "4. **Iterative Refinement**: Refines query and retries if answer is uncertain\n",
    "\n",
    "### Define State and Create Advanced Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph types\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define state that will be passed between workflow nodes\n",
    "class RAGState(TypedDict):\n",
    "    question: str                    # Original user question\n",
    "    enhanced_query: str              # Rephrased version with synonyms\n",
    "    retrieved_docs: List[Document]   # Retrieved documents\n",
    "    answer: str                      # Generated answer\n",
    "    needs_refinement: bool           # Whether answer is uncertain\n",
    "    iteration: int                   # Iteration counter\n",
    "\n",
    "# Create advanced retriever that fetches more documents\n",
    "advanced_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Workflow Nodes\n",
    "\n",
    "Each node is a function that takes the current state and returns updated state. The workflow will execute nodes in sequence, with conditional logic to decide whether to refine and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define workflow node functions\n",
    "def enhance_query(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    prompt = f\"\"\"Rephrase this query 2-3 different ways with synonyms:\n",
    "    \n",
    "    Query: {q}\n",
    "    \n",
    "    Alternatives:\"\"\"\n",
    "    \n",
    "    enhanced = llm.invoke(prompt)\n",
    "    combined = f\"{q}\\n{enhanced}\"\n",
    "    \n",
    "    print(f\"\\n[Enhance] {q[:50]}...\")\n",
    "    return {**state, \"enhanced_query\": combined}\n",
    "\n",
    "def retrieve_documents(state: RAGState) -> RAGState:\n",
    "    docs = vectorstore.similarity_search(state[\"enhanced_query\"], k=5)\n",
    "    print(f\"[Retrieve] {len(docs)} docs\")\n",
    "    return {**state, \"retrieved_docs\": docs}\n",
    "\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    \n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "    \n",
    "    Question: {q}\n",
    "    \n",
    "    Answer (say if context insufficient):\"\"\"\n",
    "    \n",
    "    answer = llm.invoke(prompt)\n",
    "    \n",
    "    # Uncertainty markers to trigger refinement\n",
    "    uncertainty = [\n",
    "        \"not sure\", \"don't know\", \"not enough\", \"unclear\", \n",
    "        \"insufficient\", \"does not mention\", \"no information\"\n",
    "    ]\n",
    "    needs_refinement = any(p in answer.lower() for p in uncertainty) and iteration < 2\n",
    "    \n",
    "    print(f\"[Generate] Iter {iteration + 1}, refine={needs_refinement}\")\n",
    "    return {**state, \"answer\": answer, \"needs_refinement\": needs_refinement, \"iteration\": iteration + 1}\n",
    "\n",
    "def refine_query(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    prev = state[\"answer\"][:200]\n",
    "    \n",
    "    prompt = f\"\"\"Question: {q}\n",
    "    Previous incomplete: {prev}\n",
    "    \n",
    "    Generate more specific query for missing info:\"\"\"\n",
    "    \n",
    "    refined = llm.invoke(prompt)\n",
    "    print(f\"[Refine] {refined[:50]}...\")\n",
    "    return {**state, \"enhanced_query\": refined}\n",
    "\n",
    "def should_refine(state: RAGState) -> str:\n",
    "    return \"refine\" if state[\"needs_refinement\"] else \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Compile Workflow\n",
    "\n",
    "Now we connect the nodes into a graph:\n",
    "- **Entry**: enhance query\n",
    "- **Flow**: enhance ‚Üí retrieve ‚Üí generate ‚Üí (if uncertain: refine ‚Üí retrieve ‚Üí generate again)\n",
    "- **Conditional edge**: Decides whether to end or refine based on answer confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Advanced RAG workflow ready\n"
     ]
    }
   ],
   "source": [
    "# Import LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Build workflow graph\n",
    "workflow = StateGraph(RAGState)\n",
    "workflow.add_node(\"enhance\", enhance_query)\n",
    "workflow.add_node(\"retrieve\", retrieve_documents)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "workflow.add_node(\"refine\", refine_query)\n",
    "\n",
    "workflow.set_entry_point(\"enhance\")\n",
    "workflow.add_edge(\"enhance\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_conditional_edges(\"generate\", should_refine, {\"refine\": \"refine\", \"end\": END})\n",
    "workflow.add_edge(\"refine\", \"retrieve\")\n",
    "\n",
    "advanced_rag = workflow.compile()\n",
    "print(\"[OK] Advanced RAG workflow ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED RAG (k=5, Query Enhancement, Refinement)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Q: I‚Äôm 70 years old, have a Gold Leaf Card and a pet dog. I want to visit the new branch on its opening day at 8:30 AM for Avocado Toast and Filter Coffee. Can I bring my dog, and what is my final cost using my credit?\n",
      "================================================================================\n",
      "\n",
      "[Enhance] I‚Äôm 70 years old, have a Gold Leaf Card and a pet ...\n",
      "[Retrieve] 5 docs\n",
      "[Generate] Iter 1, refine=False\n",
      "\n",
      "[Answer] Let's break down your question!  Here's how we can figure out the answer:\n",
      "\n",
      "* **You are 70 years old:** This means you qualify as a senior citizen.\n",
      "* **You have a Gold Leaf Card:** You're eligible for the joining credit and benefits associated with it.\n",
      "* **Visiting on opening day at 8:30 AM:**  This is great! The Early Bird Rewards apply to orders placed before 9:00 AM.\n",
      "* **Avocado Toast and Filter Coffee:** These are menu items you want to order.\n",
      "\n",
      "**Here's what we need to know to get the final cost:**\n",
      "\n",
      "1. **Is your dog allowed at the new branch?**  This information is not provided in the context, so we can't answer this yet. \n",
      "2. **How much will the Avocado Toast and Filter Coffee cost after applying the joining credit?** We need to know the price of each item to calculate the final cost.\n",
      "\n",
      "**Once you provide that information, I can help you:**\n",
      "\n",
      "* Determine if your dog is allowed at the new branch.\n",
      "* Calculate the total cost of your order using your joining credit. \n",
      "\n",
      "\n",
      "Let me know those details! üòä \n",
      "\n",
      "Iterations: 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Q: Which branch should I go to if I want to bring my pet for breakfast?\n",
      "================================================================================\n",
      "\n",
      "[Enhance] Which branch should I go to if I want to bring my ...\n",
      "[Retrieve] 5 docs\n",
      "[Generate] Iter 1, refine=False\n",
      "\n",
      "[Answer] The Indiranagar branch is pet-friendly, so you can bring your pet there for breakfast! üê∂ \n",
      "\n",
      "Iterations: 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Q: How much out-of-pocket will I pay for an Avocado Toast at Whitefield if I use my retired membership credit?\n",
      "================================================================================\n",
      "\n",
      "[Enhance] How much out-of-pocket will I pay for an Avocado T...\n",
      "[Retrieve] 5 docs\n",
      "[Generate] Iter 1, refine=False\n",
      "\n",
      "[Answer] You'll pay ‚Çπ200 less than the regular price, which is ‚Çπ450 - ‚Çπ200 = **‚Çπ250** for the Avocado Toast. \n",
      "\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The 'Bistro App' membership credit of ‚Çπ200 can be applied to your bill.\n",
      "* You are using this credit to pay for the Avocado Toast. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions! üòä \n",
      "\n",
      "Iterations: 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED RAG (k=5, Query Enhancement, Refinement)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for q in challenging_questions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    result = advanced_rag.invoke({\n",
    "        \"question\": q,\n",
    "        \"enhanced_query\": \"\",\n",
    "        \"retrieved_docs\": [],\n",
    "        \"answer\": \"\",\n",
    "        \"needs_refinement\": False,\n",
    "        \"iteration\": 0\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n[Answer] {result['answer']}\")\n",
    "    print(f\"Iterations: {result['iteration']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Have:\n",
    "\n",
    "1. **Modern LCEL Chains**: Using LangChain Expression Language (v1+ standard)\n",
    "2. **sentence-transformers**: Proper embedding model (384d, 22M params)\n",
    "3. **Qdrant**: Production vector database\n",
    "4. **LangGraph**: Stateful workflow orchestration\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Naive RAG | Advanced RAG |\n",
    "|---------|-----------|-------------|\n",
    "| Retrieval | k=2 | k=5 |\n",
    "| Query | Direct | Enhanced |\n",
    "| Refinement | None | Iterative |\n",
    "| Complex queries | Poor | Good |\n",
    "\n",
    "### Further Improvements:\n",
    "- Hybrid search\n",
    "- Intelligent chunking (hybrid or semantic)\n",
    "- Re-ranking\n",
    "- Query decomposition\n",
    "- Conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To stop Qdrant: docker stop qdrant_rag && docker rm qdrant_rag\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "# !docker stop qdrant_rag && docker rm qdrant_rag\n",
    "print(\"To stop Qdrant: docker stop qdrant_rag && docker rm qdrant_rag\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
